import sys

# Esta funci√≥n verifica que las bibliotecas necesarias est√©n instaladas.
def verificar_dependencias():
    requisitos = ['bs4', 'mechanize', 'requests']
    faltantes = []

    for paquete in requisitos:
        try:
            __import__(paquete)
        except ImportError:
            faltantes.append(paquete)

    if faltantes:
        print("\nFaltan los siguientes paquetes necesarios para ejecutar el programa:")
        for f in faltantes:
            print(f" ‚Üí {f}")
            
        print ("\nSi utiliza ü™ü Windows: ")
        print("Ejecuta este comando en la terminal:\n")
        print("'py -m pip install -r dependenciasCodigo.txt'")
        print("\nSi utiliza üêß Linux/macOS üçé")
        print("Ejecuta uno de estos comandos en la terminal:\n")
        print("'python3 -m pip install -r dependenciasCodigo.txt'")
        print("o")
        print("'python -m pip install -r dependenciasCodigo.txt'")
        print("\nSi utiliza otro Sistema Operativo use el comando adecuado para su sistema.")
 
        sys.exit("\nInstala los paquetes faltantes antes de continuar.")

# Verificaci√≥n al iniciar el programa
verificar_dependencias()

# Importaci√≥n de bibliotecas necesarias

#re permite trabajar con expresiones regulares.
import re 
#requests permite hacer peticiones HTTP para descargar el contenido de p√°ginas web.
import requests
#mechanize simula un navegador web b√°sico en Python.
import mechanize
#BeautifulSoup analiza y extrae contenido espec√≠fico del HTML
from bs4 import BeautifulSoup


# Mensaje de bienvenida al sistema
print("\nBienvenido/a al sistema de Extracci√≥n y an√°lisis de p√°ginas web.")

# Funci√≥n principal para inspeccionar una URL
# Esta funci√≥n solicita y valida una URL, luego analiza su contenido.
def inspeccionar_url():
    while True:
        buscar_url = input("\nPor favor, ingresa la URL que deseas analizar (debe comenzar con 'http://' o 'https://'): ").strip()
        if buscar_url.startswith(("http://", "https://")):
            break
        else:
            print("\nLa URL ingresada no es v√°lida. Aseg√∫rate de que comience con 'http://' o 'https://'. Intenta de nuevo ")

    # Se obtiene el HTML con requests y se usa manejo de errores.
    try:
        respuesta = requests.get(buscar_url)
        respuesta.raise_for_status()
        print("\nP√°gina obtenida exitosamente con *requests*")
        html = respuesta.text
    except requests.exceptions.RequestException as e:
        print(f"\nOcurri√≥ un error al obtener la p√°gina: {e}")
        return

    # Se usa mechanize para simular un navegador
    try:
        br = mechanize.Browser()
        br.set_handle_robots(False)
        br.addheaders = [('User-agent', 'Mozilla/5.0')]
        br.open(buscar_url)
        print("\nNavegador *mechanize* inicializado correctamente")

        # Se inicia mechanize y se abre la p√°gina.
        print("\nExplorando enlaces con mechanize...\n")
        for i, link in enumerate(br.links()):
            if i >= 5:
                break
            try:
                print(f"Enlace {i+1}: {link.url}")
                br.follow_link(link)
                pagina = br.response().read()
                pagina_soup = BeautifulSoup(pagina, 'html.parser')
                title = pagina_soup.title.string if pagina_soup.title else "Sin t√≠tulo"
                print("T√≠tulo:", title)
                print("-" * 40)
                br.back()
            except Exception as e:
                print(f"\nLo siento, no se pudo seguir el enlace: {e}")

    except Exception as e:
        print(f"\nLo siento,No se pudo abrir la p√°gina con mechanize: {e}")

    # Se extraen los enlaces del HTML utilizando BeautifulSoup.
    soup = BeautifulSoup(html, 'html.parser')
    links = [link.get('href') for link in soup.find_all('a', href=True)]
    print("\nEnlaces encontrados en la p√°gina con BeautifulSoup:")
    if links:
        for link in links[:10]:
            print(" -", link)
    else:
        print("\nLo siento, no se encontraron enlaces en esta p√°gina.")

    # Extrae los primeros p√°rrafos.
    parrafos = soup.find_all('p')
    print("\nPrimeros p√°rrafos encontrados:")
    if parrafos:
        for p in parrafos[:5]:
            print(" ‚Üí", p.get_text(strip=True))
    else:
        print("\nLo siento, no se encontraron p√°rrafos en esta p√°gina.")

    # Busqueda de correos
    # La siguente l√≠nea busca todos los textos en sub_html que tengan el formato de un correo electr√≥nico y devuelve una lista con todos ellos.
    emails = re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", html)
    print("\nCorreos electr√≥nicos detectados:")
    if emails:
        for email in emails:
            print(" ‚Üí", email)
    else:
        print("\nLo siento, no se encontraron correos en la p√°gina principal.")

    #Se analizan los primeros enlaces encontrados de las subp√°ginas y se extrae informaci√≥n √∫til.
    print("\nAnalizando algunas subp√°ginas...\n")
    for i, link in enumerate(links[:5]):
        full_url = link if link.startswith('http') else buscar_url + link
        try:
            sub_respuesta = requests.get(full_url)
            sub_respuesta.raise_for_status()
            sub_html = sub_respuesta.text
            sub_soup = BeautifulSoup(sub_html, 'html.parser')
            print(f"Subp√°gina {i+1}: {full_url}")

            # T√≠tulo
            titulo = sub_soup.title.string if sub_soup.title else "Sin t√≠tulo"
            print("T√≠tulo:", titulo)

            # Primer p√°rrafo
            sub_parrafo = sub_soup.find('p')
            if sub_parrafo:
                print("Primer p√°rrafo:", sub_parrafo.get_text(strip=True))
            else:
                print("\nLo siento, no se encontr√≥ ning√∫n p√°rrafo.")

            # Correos en subp√°gina
            sub_emails = re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", sub_html)
            if sub_emails:
                print("Correos encontrados:")
                for email in sub_emails:
                    print("    ‚Üí", email)
            else:
                print("\nLo siento, no se encontraron correos.")

            print("-" * 50)
        except requests.exceptions.RequestException as e:
            print(f"\nNo se pudo acceder a {full_url}: {e}")

# Funci√≥n que permite controlar el flujo del programa y consulta si desea analizar otro sitio web o no.
# Si el usuario no desea continuar se indica el final del programa.
# Se valida que la opci√≥n est√© correcta, si no lo est√° vuelve al ciclo y pregunta de nuevo. 
def main():
    while True:
        inspeccionar_url()
        while True:
            seguir = input("\n¬øTe gustar√≠a analizar otra URL? (S/N): ").upper()
            if seguir == "S":
                break
            elif seguir == "N":
                print("\nGracias por usar el sistema. ¬°Hasta pronto!")
                return
            else:
                print("\nPor favor, ingresa una opci√≥n v√°lida: S/N ")

# Ejecuci√≥n principal
main()